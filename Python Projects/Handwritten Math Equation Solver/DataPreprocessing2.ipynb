{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import imageio\n",
    "from PIL import Image,ImageOps\n",
    "from random import sample\n",
    "import hashlib\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:27: SyntaxWarning: invalid escape sequence '\\A'\n",
      "<>:27: SyntaxWarning: invalid escape sequence '\\A'\n",
      "C:\\Users\\ahmed\\AppData\\Local\\Temp\\ipykernel_21228\\4109754904.py:27: SyntaxWarning: invalid escape sequence '\\A'\n",
      "  root_directory = \"D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\(\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\)\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\+\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\-\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\0\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\1\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\2\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\3\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\4\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\5\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\6\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\7\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\8\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\9\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\=\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\e\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\X\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\y\n",
      "Processing Directory: D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\\crohme\\z\n",
      "Total Files: 54477\n",
      "Total Duplicates Removed: 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def delete_duplicates(root_path):\n",
    "    print('Root Directory:', root_path)\n",
    "\n",
    "    total_files = 0\n",
    "    total_duplicates = 0\n",
    "    file_hashes = {}\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        print('Processing Directory:', dirpath)\n",
    "        for filename in filenames:\n",
    "            total_files += 1\n",
    "            if filename.endswith('.jpg') and not filename.startswith('._'):\n",
    "                file_path = os.path.join(dirpath, filename)\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    file_data = f.read()\n",
    "                file_hash = hashlib.md5(file_data).hexdigest()\n",
    "\n",
    "                if file_hash in file_hashes:\n",
    "                    total_duplicates += 1\n",
    "                    print('Duplicate found:', filename)\n",
    "                    os.remove(file_path)\n",
    "                else:\n",
    "                    file_hashes[file_hash] = file_path\n",
    "\n",
    "    print('Total Files:', total_files)\n",
    "    print('Total Duplicates Removed:', total_duplicates)\n",
    "\n",
    "root_directory = \"D:\\Ahmed\\AI\\OCR-Math\\Data\\crohme\"\n",
    "delete_duplicates(root_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_mapping(mapping_raw, mapping_processed, extra_chars):\n",
    "\n",
    "    df = pd.read_csv(mapping_raw, sep=' ', header=None, names=[\"id\", \"code\"])\n",
    "    df[\"char\"] = df[\"code\"].apply(chr)\n",
    "  \n",
    "    nextId = df.shape[0]\n",
    "\n",
    "    for i, c in enumerate(extra_chars, start=nextId):\n",
    "        df.loc[i] = [i, ord(c), c]\n",
    "  \n",
    "    df.to_csv(mapping_processed, index=False)\n",
    "\n",
    "mapping_raw = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Original-EMNIST-Dataset\\\\emnist-balanced-mapping.txt\"\n",
    "mapping_processed = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping.csv\"\n",
    "extra_chars = ['(', ')', '+', '-', '=']\n",
    "\n",
    "process_mapping(mapping_raw, mapping_processed, extra_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#images from the emnist dataset are flipped horizontally and rotated 90 degrees.\n",
    "#These functions are used to adjust the images to their original orientation.\n",
    "#The pixel values are also normalized to the range [0, 1]\n",
    "\n",
    "\n",
    "def mirror(X):\n",
    "    res = np.zeros(X.shape)\n",
    "    n = 28\n",
    "    for r in range(n, n**2 + 1, n):\n",
    "        l = r - n\n",
    "        for k in range(l, r):\n",
    "            index = l + (r - k - 1)\n",
    "            res[:,k] = X[:,index]\n",
    "            \n",
    "    return res        \n",
    "def rotate_clockwise(X):\n",
    "    res = np.zeros(X.shape)\n",
    "    size = 28\n",
    "    k = 0\n",
    "    for i in reversed(range(size)):\n",
    "        j = i\n",
    "        while j < size**2:\n",
    "            res[:,k] = X[:,j]\n",
    "            k += 1\n",
    "            j += size\n",
    "\n",
    "    return res\n",
    "\n",
    "def rotate(X, times):\n",
    "    for i in range(times):\n",
    "        X = rotate_clockwise(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(file_from, file_to):\n",
    "    label = 'label'\n",
    "    names = [label] + [\"px\" + str(i) for i in range(784)]\n",
    "    data = pd.read_csv(file_from, header=None, names=names)\n",
    "    \n",
    "    Y_data = data[label]\n",
    "    X_data = data.drop(labels = [label], axis = 1)\n",
    "    \n",
    "    X_data = X_data / 255\n",
    "    X_data = np.where(X_data > 0.5, 1, 0)\n",
    "    X_data = rotate(X_data, 3)\n",
    "    X_data = mirror(X_data)\n",
    "    \n",
    "    data = pd.DataFrame(X_data, columns=names[1:], dtype='int')    \n",
    "    data.insert(0, label, Y_data)        \n",
    "    data.to_csv(file_to, index=False)\n",
    "\n",
    "raw_training_cv = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Original-EMNIST-Dataset\\\\emnist-balanced-train.csv\"\n",
    "processed_training = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_training.csv\"\n",
    "raw_testing_cv = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Original-EMNIST-Dataset\\\\emnist-balanced-test.csv\"\n",
    "processed_testing = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_testing.csv\"\n",
    "process_data(raw_training_cv, processed_training)\n",
    "process_data(raw_testing_cv, processed_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_mappings_with_ids(csv_file_path, ids_to_remove):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Remove entries with the specified IDs\n",
    "    df = df[~df['id'].isin(ids_to_remove)]\n",
    "    \n",
    "    # Save the filtered DataFrame back to a CSV file\n",
    "    filtered_csv_file_path = csv_file_path.replace('.csv', '_filtered.csv')\n",
    "    df.to_csv(filtered_csv_file_path, index=False)\n",
    "    \n",
    "    return filtered_csv_file_path\n",
    "\n",
    "csv_file_path = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping.csv\"\n",
    "ids_to_remove = [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,36,37,38,40,41,42,43,44,45,46]\n",
    "filtered_csv_file_path = remove_mappings_with_ids(csv_file_path, ids_to_remove)\n",
    "print(f\"Filtered CSV file saved to: {filtered_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def renumber_labels(csv_file_path):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Create a mapping dictionary to map old IDs to new IDs\n",
    "    mapping_dict = {old_id: new_id for new_id, old_id in enumerate(df['id'])}\n",
    "    \n",
    "    # Update the 'id' column in the DataFrame using the mapping dictionary\n",
    "    df['id'] = df['id'].map(mapping_dict)\n",
    "    \n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    updated_csv_file_path = csv_file_path.replace('.csv', '_renumbered.csv')\n",
    "    df.to_csv(updated_csv_file_path, index=False)\n",
    "    \n",
    "    return updated_csv_file_path\n",
    "\n",
    "\n",
    "csv_file_path = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping_filtered.csv\"\n",
    "renumbered_csv_file_path = renumber_labels(csv_file_path)\n",
    "print(f\"Renumbered CSV file saved to: {renumbered_csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def remove_rows_with_labels(csv_file_path, labels_to_remove):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file_path)\n",
    "    \n",
    "    # Remove rows with the specified labels\n",
    "    for label in labels_to_remove:\n",
    "        df = df[df['label'] != label]\n",
    "    \n",
    "    # Save the filtered DataFrame back to a CSV file\n",
    "    filtered_csv_file_path = csv_file_path.replace('.csv', '_filtered.csv')\n",
    "    df.to_csv(filtered_csv_file_path, index=False)\n",
    "    \n",
    "    return filtered_csv_file_path\n",
    "\n",
    "csv_file_testing = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_testing.csv\"\n",
    "csv_file_training = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_training.csv\"\n",
    "labels_to_remove = [10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,36,37,38,40,41,42,43,44,45,46]\n",
    "filtered_csv_file_path = remove_rows_with_labels(csv_file_testing, labels_to_remove)\n",
    "filtered_csv_file_path = remove_rows_with_labels(csv_file_training, labels_to_remove)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def map_old_to_new_labels(old_mapping_file_path, new_mapping_file_path, image_csv_file_path):\n",
    "    # Read the old and new mapping CSV files\n",
    "    old_mapping_df = pd.read_csv(old_mapping_file_path)\n",
    "    new_mapping_df = pd.read_csv(new_mapping_file_path)\n",
    "    \n",
    "    # Merge the old and new mapping DataFrames on the \"code\" column\n",
    "    merged_df = pd.merge(old_mapping_df, new_mapping_df, on='code', suffixes=('_old', '_new'))\n",
    "    \n",
    "    # Create a dictionary to map old labels to new labels\n",
    "    mapping_dict = dict(zip(merged_df['id_old'], merged_df['id_new']))\n",
    "    \n",
    "    # Read the image pixel CSV file\n",
    "    image_df = pd.read_csv(image_csv_file_path)\n",
    "    \n",
    "    # Map old labels to new labels based on the mapping dictionary\n",
    "    image_df['label'] = image_df['label'].map(mapping_dict)\n",
    "    \n",
    "    # Save the updated DataFrame back to a CSV file\n",
    "    updated_csv_file_path = image_csv_file_path.replace('.csv', '_mapped.csv')\n",
    "    image_df.to_csv(updated_csv_file_path, index=False)\n",
    "    \n",
    "    return updated_csv_file_path\n",
    "\n",
    "old_mapping_file_path = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping_filtered.csv\"\n",
    "new_mapping_file_path = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping_filtered_renumbered.csv\"\n",
    "image_csv_file_path = \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_testing_filtered.csv\"\n",
    "mapped_image_csv_file_path = map_old_to_new_labels(old_mapping_file_path, new_mapping_file_path, image_csv_file_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_mapping_filtered_renumbered.csv\")\n",
    "char2code = {}\n",
    "for index, row in df.iterrows():\n",
    "    char2code[row['char']] = row['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_csv(filepath, char_code):\n",
    "    img = cv2.imread(filepath, 0)\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    dilation = cv2.erode(img, kernel, iterations=1)\n",
    "    \n",
    "    img = Image.fromarray(dilation).resize((28, 28))\n",
    "    inv_img = ImageOps.invert(img)\n",
    "    \n",
    "    flattened = np.array(inv_img).flatten()\n",
    "    flattened = flattened / 255\n",
    "    flattened = np.where(flattened > 0.5, 1, 0)\n",
    "    \n",
    "    csv_img = ','.join([str(num) for num in flattened])\n",
    "    csv_str = '{},{}'.format(char_code, csv_img)\n",
    "    return csv_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing \"(\" character...\n",
      "Processing \")\" character...\n",
      "Processing \"+\" character...\n",
      "Processing \"-\" character...\n",
      "Processing \"=\" character...\n"
     ]
    }
   ],
   "source": [
    "train_data_size = 2400\n",
    "test_data_size = 400\n",
    "f_test_output = open(\"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_testing_filtered_mapped.csv\", 'a')\n",
    "f_train_output = open(\"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\emnist\\\\Processed-Dataset\\\\processed_training_filtered_mapped.csv\", 'a')\n",
    "\n",
    "for c in extra_chars:\n",
    "    print('Processing \"{}\" character...'.format(c))\n",
    "    current_directory =  \"D:\\\\Ahmed\\\\AI\\\\OCR-Math\\\\Data\\\\crohme\\\\crohme\\\\\" + c + '\\\\'\n",
    "    files = [file_name for dir_path, dir_name, file_name in os.walk(current_directory)]\n",
    "    subset = sample(files[0], train_data_size + test_data_size)\n",
    "    train_subset = subset[0:train_data_size]\n",
    "    test_subset = subset[train_data_size:train_data_size + test_data_size]\n",
    "    \n",
    "    for filename in train_subset:\n",
    "        csv_string = convert_image_to_csv(current_directory + filename, char2code[c])\n",
    "        print(csv_string, file=f_train_output)\n",
    "    \n",
    "    for filename in test_subset:\n",
    "        csv_string = convert_image_to_csv(current_directory + filename, char2code[c])\n",
    "        print(csv_string, file=f_test_output)\n",
    "\n",
    "f_test_output.close()\n",
    "f_train_output.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
